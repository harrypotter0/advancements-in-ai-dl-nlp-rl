{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/42605769/openai-gym-atari-on-windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Create a new notebook for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import time\n",
    "import random\n",
    "\n",
    "import argparse\n",
    "from pygame.locals import *\n",
    "from PIL import Image\n",
    "# from Logger import Logger\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNetwork:\n",
    "    def __init__(self, actions, input_shape, alpha=0.1, gamma=0.99,\n",
    "                 dropout_prob=0.1, load_path='', logger=None):\n",
    "        self.model = Sequential()\n",
    "        self.actions = actions  # Size of the network output\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        # Define neural network\n",
    "        self.model.add(BatchNormalization(axis=1, input_shape=input_shape))\n",
    "        self.model.add(Convolution2D(32, 2, 2, border_mode='valid',\n",
    "                                     subsample=(2, 2), dim_ordering='th'))\n",
    "        self.model.add(Activation('relu'))\n",
    "\n",
    "        self.model.add(BatchNormalization(axis=1))\n",
    "        self.model.add(Convolution2D(64, 2, 2, border_mode='valid',\n",
    "                                     subsample=(2, 2), dim_ordering='th'))\n",
    "        self.model.add(Activation('relu'))\n",
    "\n",
    "        self.model.add(BatchNormalization(axis=1))\n",
    "        self.model.add(Convolution2D(64, 3, 3, border_mode='valid',\n",
    "                                     subsample=(2, 2), dim_ordering='th'))\n",
    "        self.model.add(Activation('relu'))\n",
    "\n",
    "        self.model.add(Flatten())\n",
    "\n",
    "        self.model.add(Dropout(self.dropout_prob))\n",
    "        self.model.add(Dense(512))\n",
    "        self.model.add(Activation('relu'))\n",
    "\n",
    "        self.model.add(Dense(self.actions))\n",
    "\n",
    "        self.optimizer = Adam()\n",
    "        self.logger = logger\n",
    "\n",
    "        # Load the network from saved model\n",
    "        if load_path != '':\n",
    "            self.load(load_path)\n",
    "\n",
    "        self.model.compile(loss='mean_squared_error', optimizer=self.optimizer,\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    def train(self, batch):\n",
    "        \"\"\"\n",
    "        Generates inputs and targets from the given batch, trains the model on\n",
    "        them.\n",
    "        :param batch: iterable of dictionaries with keys 'source', 'action',\n",
    "        'dest', 'reward'\n",
    "        \"\"\"\n",
    "        x_train = []\n",
    "        t_train = []\n",
    "\n",
    "        # Generate training set and targets\n",
    "        for datapoint in batch:\n",
    "            x_train.append(datapoint['source'].astype(np.float64))\n",
    "\n",
    "            # Get the current Q-values for the next state and select the best\n",
    "            next_state_pred = self.predict(datapoint['dest'].astype(np.float64)).ravel()\n",
    "            next_q_value = np.max(next_state_pred)\n",
    "\n",
    "            # The error must be 0 on all actions except the one taken\n",
    "            t = list(self.predict(datapoint['source'])[0])\n",
    "            if datapoint['final']:\n",
    "                t[datapoint['action']] = datapoint['reward']\n",
    "            else:\n",
    "                t[datapoint['action']] = datapoint['reward'] + \\\n",
    "                                         self.gamma * next_q_value\n",
    "\n",
    "            t_train.append(t)\n",
    "\n",
    "        # Prepare inputs and targets\n",
    "        x_train = np.asarray(x_train).squeeze()\n",
    "        t_train = np.asarray(t_train).squeeze()\n",
    "\n",
    "        # Train the model for one epoch\n",
    "        h = self.model.fit(x_train,\n",
    "                           t_train,\n",
    "                           batch_size=32,\n",
    "                           nb_epoch=1)\n",
    "\n",
    "        # Log loss and accuracy\n",
    "        if self.logger is not None:\n",
    "            self.logger.to_csv('loss_history.csv',\n",
    "                               [h.history['loss'][0], h.history['acc'][0]])\n",
    "\n",
    "    def predict(self, state):\n",
    "        \"\"\"\n",
    "        Feeds state into the model, returns predicted Q-values.\n",
    "        :param state: a numpy.array with same shape as the network's input\n",
    "        :return: numpy.array with predicted Q-values\n",
    "        \"\"\"\n",
    "        state = state.astype(np.float64)\n",
    "        return self.model.predict(state, batch_size=1)\n",
    "\n",
    "    def save(self, filename=None):\n",
    "        \"\"\"\n",
    "        Saves the model weights to disk.\n",
    "        :param filename: file to which save the weights (must end with \".h5\")\n",
    "        \"\"\"\n",
    "        f = ('model.h5' if filename is None else filename)\n",
    "        if self.logger is not None:\n",
    "            self.logger.log('Saving model as %s' % f)\n",
    "        self.model.save_weights(self.logger.path + f)\n",
    "\n",
    "    def load(self, path):\n",
    "        \"\"\"\n",
    "        Loads the model's weights from path.\n",
    "        :param path: h5 file from which to load teh weights\n",
    "        \"\"\"\n",
    "        if self.logger is not None:\n",
    "            self.logger.log('Loading weights from file...')\n",
    "        self.model.load_weights(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQAgent:\n",
    "    def __init__(self,\n",
    "                 actions,\n",
    "                 batch_size=1024,\n",
    "                 alpha=0.01,\n",
    "                 gamma=0.9,\n",
    "                 dropout_prob=0.1,\n",
    "                 epsilon=1,\n",
    "                 epsilon_rate=0.99,\n",
    "                 network_input_shape=(2, 84, 84),\n",
    "                 load_path='',\n",
    "                 logger=None):\n",
    "\n",
    "        # Parameters\n",
    "        self.actions = actions  # Size of the discreet action space\n",
    "        self.batch_size = batch_size  # Size of the batch to train the network\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Coefficient for epsilon-greedy exploration\n",
    "        self.epsilon_rate = epsilon_rate  # Rate at which to make epsilon smaller, as training improves the agent's performance; epsilon = epsilon * rate\n",
    "        self.min_epsilon = 0.3  # Minimum epsilon value\n",
    "        # Experience variables\n",
    "        self.experiences = []\n",
    "        self.training_count = 0\n",
    "\n",
    "        # Instantiate the deep Q-network\n",
    "        self.DQN = DQNetwork(\n",
    "            self.actions,\n",
    "            network_input_shape,\n",
    "            alpha=alpha,\n",
    "            gamma=self.gamma,\n",
    "            dropout_prob=dropout_prob,\n",
    "            load_path=load_path,\n",
    "            logger=logger\n",
    "        )\n",
    "\n",
    "        if logger is not None:\n",
    "            logger.log({\n",
    "                'Learning rate': alpha,\n",
    "                'Discount factor': self.gamma,\n",
    "                'Starting epsilon': self.epsilon,\n",
    "                'Epsilon decrease rate': self.epsilon_rate,\n",
    "                'Batch size': self.batch_size\n",
    "            })\n",
    "\n",
    "    def get_action(self, state, testing=False):\n",
    "        \"\"\"\n",
    "        Poll DCN for Q-values, return greedy action with probability 1-epsilon\n",
    "        :param state: a state of the MDP with the same size as the DQN input\n",
    "        :param testing: whether to force a greedy action\n",
    "        :return: the selected action\n",
    "        \"\"\"\n",
    "        q_values = self.DQN.predict(state)\n",
    "        if (random.random() < self.epsilon) and not testing:\n",
    "            return random.randint(0, self.actions - 1)\n",
    "        else:\n",
    "            return np.argmax(q_values)\n",
    "\n",
    "    def add_experience(self, source, action, reward, dest, final):\n",
    "        \"\"\"\n",
    "        Add a tuple (source, action, reward, dest, final) to experiences.\n",
    "        :param source: a state of the MDP\n",
    "        :param action: the action associated to the transition\n",
    "        :param reward: the reward associated to the transition\n",
    "        :param dest: a state of the MDP\n",
    "        :param final: whether the destination state is an absorbing state\n",
    "        \"\"\"\n",
    "        self.experiences.append({'source': source,\n",
    "                                 'action': action,\n",
    "                                 'reward': reward,\n",
    "                                 'dest': dest,\n",
    "                                 'final': final})\n",
    "\n",
    "    def sample_batch(self):\n",
    "        \"\"\"\n",
    "        Pops self.batch_size random samples from experiences and return them as\n",
    "        a batch.\n",
    "        \"\"\"\n",
    "        out = [self.experiences.pop(random.randrange(0, len(self.experiences)))\n",
    "               for _ in range(self.batch_size)]\n",
    "        return np.asarray(out)\n",
    "\n",
    "    def must_train(self):\n",
    "        \"\"\"\"\n",
    "        Returns true if the number of samples in experiences is greater than the\n",
    "        batch size.\n",
    "        \"\"\"\n",
    "        return len(self.experiences) >= self.batch_size\n",
    "\n",
    "    def train(self, update_epsilon=True):\n",
    "        \"\"\"\n",
    "        Samples a batch from experiences, trains the DQN on it, and updates the\n",
    "        epsilon-greedy coefficient.\n",
    "        \"\"\"\n",
    "        self.training_count += 1\n",
    "        print ('Training session #', self.training_count, ' - epsilon:', self.epsilon)\n",
    "        batch = self.sample_batch()\n",
    "        self.DQN.train(batch)  # Train the DQN\n",
    "        if update_epsilon:\n",
    "            self.epsilon = self.epsilon * self.epsilon_rate if self.epsilon > self.min_epsilon else self.min_epsilon  # Decrease the probability of picking a random action to improve exploitation\n",
    "\n",
    "    def quit(self):\n",
    "        \"\"\"\n",
    "        Saves the DQN to disk.\n",
    "        \"\"\"\n",
    "        self.DQN.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.init()\n",
    "\n",
    "display_width = 600\n",
    "display_height = 400\n",
    "\n",
    "fps = 30\n",
    "white = (255,255,255)\n",
    "black = (0,0,0)\n",
    "red = (255,0,0)\n",
    "green = (0,155,0)\n",
    "\n",
    "direction = \"right\"\n",
    "actions = 4\n",
    "\n",
    "icon = pygame.image.load('Assets/appleImg.png')\n",
    "img = pygame.image.load('Assets/snakehead.png')\n",
    "appleimg = pygame.image.load('Assets/apple.png')\n",
    "\n",
    "gameDisplay = pygame.display.set_mode((display_width,display_height))\n",
    "pygame.display.set_caption('Snake')\n",
    "pygame.display.set_icon(icon)\n",
    "\n",
    "block_size = 10\n",
    "AppleThickness = 20\n",
    "snake_speed = 5\n",
    "apple_reward = 1\n",
    "life_reward = 0\n",
    "death_reward = -1\n",
    "\n",
    "clock = pygame.time.Clock()\n",
    "\n",
    "smallfont = pygame.font.SysFont('Trebuchet MS', 14)\n",
    "medfont = pygame.font.SysFont('Trebuchet MS', 24)\n",
    "largefont = pygame.font.SysFont('Trebuchet MS', 52)\n",
    "\n",
    "# CONSTANTS\n",
    "MAX_EPISODE_LENGTH_FACTOR = 100\n",
    "MAX_EPISODES_BETWEEN_TRAININGS = 1500\n",
    "SCREENSHOT_DIMS = (84, 84)\n",
    "\n",
    "global_episode_counter = 0\n",
    "exp_backup_counter = 0\n",
    "experience_buffer = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_game():\n",
    "    global direction, \\\n",
    "            episode_length, \\\n",
    "            episode_reward, \\\n",
    "            action, state, next_state, must_die, \\\n",
    "            lead_x, lead_y, lead_x_change, lead_y_change, snakeList, snakeLength, \\\n",
    "            randAppleX, randAppleY\n",
    "    \n",
    "    direction = random.choice([\"right\", \"left\", \"up\", \"down\"])\n",
    "    action = random.randint(0, actions - 1) # 0-left 1-right 2-up 3-down\n",
    "    \n",
    "    must_die = False\n",
    "    \n",
    "    # Stats\n",
    "    episode_length = 0\n",
    "    episode_reward = 0\n",
    "#     episode_nb = 0\n",
    "#     exp_backup_counter = 0\n",
    "#     global_episode_counter = 0  # Keeps track of how many episodes there were between traning iterations\n",
    "    \n",
    "    gameExit = False\n",
    "    gameOver = False\n",
    "    \n",
    "    # Start position and movement\n",
    "    lead_x = display_width / 2\n",
    "    lead_y = display_height / 2\n",
    "    lead_x_change, lead_y_change = init_movement(direction)\n",
    "\n",
    "    snakeList = []\n",
    "    snakeLength = 1\n",
    "\n",
    "    randAppleX, randAppleY = randAppleGen()\n",
    "    \n",
    "    # Initialize the states\n",
    "    state = [screenshot(), screenshot()]\n",
    "    next_state = [screenshot(), screenshot()]\n",
    "    \n",
    "    gameDisplay.fill(white)\n",
    "    pygame.display.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def game_intro():\n",
    "    intro = True\n",
    "\n",
    "    while intro:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                quit()\n",
    "            if event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_SPACE:\n",
    "                    intro = False\n",
    "                if event.key == pygame.K_q:\n",
    "                    pygame.quit()\n",
    "                    quit()\n",
    "\n",
    "\n",
    "        gameDisplay.fill(white)\n",
    "        message_to_screen(\"Snake\", green, -100, \"large\")\n",
    "        message_to_screen(\"Eat the Red apples to grow as much as you can!\", black, -30)\n",
    "        message_to_screen(\"Make sure you do not to run into yourself, or the edges.\", black, 10)\n",
    "        message_to_screen(\"Press 'Space' to play/pause or 'Q' to quit\", red, 50)\n",
    "\n",
    "        pygame.display.update()\n",
    "        clock.tick(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snake(block_size, snakeList):\n",
    "    if direction == \"right\":\n",
    "        head = pygame.transform.rotate(img, 270)\n",
    "    if direction == \"left\":\n",
    "        head = pygame.transform.rotate(img, 90)\n",
    "    if direction == \"up\":\n",
    "        head = pygame.transform.rotate(img, 0)\n",
    "    if direction == \"down\":\n",
    "        head = pygame.transform.rotate(img, 180)\n",
    "\n",
    "    gameDisplay.blit(head, (snakeList[-1][0], snakeList[-1][1]))\n",
    "    for XnY in snakeList[:-1]:\n",
    "        pygame.draw.rect(gameDisplay, green, [XnY[0],XnY[1],block_size,block_size])\n",
    "\n",
    "def randAppleGen():\n",
    "    randAppleX = round(random.randrange(0, display_width - AppleThickness) / 10.0) * 10.0\n",
    "    randAppleY = round(random.randrange(0, display_height - AppleThickness) / 10.0) * 10.0\n",
    "\n",
    "    return randAppleX, randAppleY\n",
    "\n",
    "def init_movement(direction):\n",
    "    if direction == \"right\":\n",
    "        lead_x_change = 5\n",
    "        lead_y_change = 0\n",
    "    elif direction == \"left\":\n",
    "        lead_x_change = -5\n",
    "        lead_y_change = 0\n",
    "    elif direction == \"up\":\n",
    "        lead_x_change = 0\n",
    "        lead_y_change = -5\n",
    "    elif direction == \"down\":\n",
    "        lead_x_change = 0\n",
    "        lead_y_change = 5\n",
    "        \n",
    "    return lead_x_change, lead_y_change\n",
    "\n",
    "def Score(score):\n",
    "    text = smallfont.render(\"Score: \"+str(score), True, black)\n",
    "    gameDisplay.blit(text, [0,0])\n",
    "\n",
    "def text_objects(text, color, size):\n",
    "    if size == \"small\":\n",
    "        textSurface = smallfont.render(text, True, color)\n",
    "    if size == \"medium\":\n",
    "        textSurface = medfont.render(text, True, color)\n",
    "    if size == \"large\":\n",
    "        textSurface = largefont.render(text, True, color)\n",
    "    return textSurface, textSurface.get_rect()\n",
    "    \n",
    "def message_to_screen(msg, color, y_displace=0, size = \"small\"):\n",
    "    textSurf, textRect = text_objects(msg, color, size)\n",
    "    textRect.center = (display_width / 2), (display_height / 2)+y_displace\n",
    "    gameDisplay.blit(textSurf, textRect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def die(DQA):\n",
    "    global logger, remaining_iters, episode_length, episode_reward, \\\n",
    "        must_test, experience_buffer, exp_backup_counter, global_episode_counter\n",
    "\n",
    "#     set_trace()\n",
    "    global_episode_counter += 1\n",
    "\n",
    "    # If agent is stuck, kill the process\n",
    "    if global_episode_counter > MAX_EPISODES_BETWEEN_TRAININGS:\n",
    "        print('Shutting process down because something seems to have gone '\n",
    "                   'wrong during training. Please manually check that '\n",
    "                   'all is OK and restart the training with the -l flag.')\n",
    "        DQA.quit()\n",
    "        sys.exit(0)\n",
    "\n",
    "    # Before resetting must_test, save info about the test episode\n",
    "#     if must_test:\n",
    "#         logger.to_csv('test_data.csv', [score, episode_length, episode_reward])\n",
    "#         logger.log('Test episode - Score: %s; Steps: %s'\n",
    "#                    % (score, episode_length))\n",
    "\n",
    "    # Reset this every time (only one testing episode per training session)\n",
    "#     must_test = False\n",
    "\n",
    "    # Add the episode to the experience buffer\n",
    "    if snakeLength >= 1 and episode_length >= 10:\n",
    "        exp_backup_counter += len(experience_buffer)\n",
    "        print ('Adding episode to experiences - Score: %s; Episode length: %s' \\\n",
    "              % (snakeLength, episode_length))\n",
    "#         logger.to_csv('train_data.csv', [snakeLength, episode_length, episode_reward])\n",
    "        print ('Got %s samples of %s' % (exp_backup_counter, DQA.batch_size))\n",
    "        for exp in experience_buffer:\n",
    "            DQA.add_experience(*exp)\n",
    "\n",
    "    # Train the network\n",
    "#     if DQA.must_train() and args.train:\n",
    "    if DQA.must_train():\n",
    "        exp_backup_counter = 0\n",
    "        print('Episodes elapsed: %d' % global_episode_counter)\n",
    "        global_episode_counter = 0\n",
    "        # Quit at the last iteration\n",
    "        if remaining_iters == 0:\n",
    "            DQA.quit()\n",
    "            sys.exit(0)\n",
    "\n",
    "        # Train the DQN\n",
    "        DQA.train()\n",
    "\n",
    "        remaining_iters -= 1 if remaining_iters != -1 else 0\n",
    "        # After training, the next episode will be a test one\n",
    "#         must_test = True\n",
    "#         logger.log('Test episode')\n",
    "\n",
    "    experience_buffer = []\n",
    "\n",
    "    # Update graphics and restart episode\n",
    "    pygame.display.update()\n",
    "    init_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def screenshot():\n",
    "    \"\"\"\n",
    "    Takes a screenshot of the game, converts it to greyscale, resizes it to\n",
    "    60x60 and returns it as np.array\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global gameDisplay\n",
    "    data = pygame.image.tostring(gameDisplay, 'RGB')  # Take screenshot\n",
    "    image = Image.frombytes('RGB', (display_width, display_height), data)\n",
    "    image = image.convert('L')  # Convert to greyscale\n",
    "    image = image.resize(SCREENSHOT_DIMS)  # Resize\n",
    "    image = image.convert('1')\n",
    "    matrix = np.asarray(image.getdata(), dtype=np.float64)\n",
    "    return matrix.reshape(image.size[0], image.size[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gameLoop():\n",
    "    global direction, snakeLength, \\\n",
    "            episode_length, \\\n",
    "            episode_reward, \\\n",
    "            action, state, next_state, must_die\n",
    "    \n",
    "    direction = random.choice([\"right\", \"left\", \"up\", \"down\"])\n",
    "    action = random.randint(0, actions - 1) # 0-left 1-right 2-up 3-down\n",
    "    \n",
    "    must_die = False\n",
    "    \n",
    "    # Stats\n",
    "    episode_length = 0\n",
    "    episode_reward = 0\n",
    "    episode_nb = 0\n",
    "    exp_backup_counter = 0\n",
    "    global_episode_counter = 0  # Keeps track of how many episodes there were between traning iterations\n",
    "    \n",
    "    gameExit = False\n",
    "    gameOver = False\n",
    "    \n",
    "    # Start position and movement\n",
    "    lead_x = display_width / 2\n",
    "    lead_y = display_height / 2\n",
    "    lead_x_change, lead_y_change = init_movement(direction)\n",
    "\n",
    "    snakeList = []\n",
    "    snakeLength = 1\n",
    "\n",
    "    randAppleX, randAppleY = randAppleGen()\n",
    "    \n",
    "    # Agent\n",
    "    DQA = DQAgent(\n",
    "        actions,\n",
    "        gamma=0.95,\n",
    "        dropout_prob=0.1,\n",
    "        load_path=''\n",
    "    )\n",
    "    experience_buffer = []  # This will store the SARS tuples at each episode\n",
    "    \n",
    "    # Initialize the states\n",
    "    state = [screenshot(), screenshot()]\n",
    "    next_state = [screenshot(), screenshot()]\n",
    "    \n",
    "#     gameDisplay.fill(white)\n",
    "#     pygame.display.update()\n",
    "    \n",
    "    while not gameExit:\n",
    "        episode_length += 1\n",
    "        reward = life_reward # Reward for not dying and not eating\n",
    "        next_state[0] = state[1]\n",
    "        \n",
    "        clock.tick(fps)\n",
    "        \n",
    "        while gameOver == True:\n",
    "            gameDisplay.fill(white)\n",
    "            message_to_screen('Game Over!', red, -50, \"medium\")\n",
    "            message_to_screen('Press C to play again or Q to quit', black, 50, \"small\")\n",
    "            pygame.display.update()\n",
    "\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == QUIT:\n",
    "                    gameExit = True\n",
    "                    gameOver = False\n",
    "                    DQA.quit()\n",
    "                    sys.exit(0)\n",
    "        \n",
    "        if action == 0 and direction != \"right\":\n",
    "            direction = \"left\"\n",
    "            lead_x_change = -snake_speed\n",
    "            lead_y_change = 0\n",
    "        elif action == 1 and direction != \"left\":\n",
    "            direction = \"right\"\n",
    "            lead_x_change = snake_speed\n",
    "            lead_y_change = 0\n",
    "        elif action == 2 and direction != \"down\":\n",
    "            direction = \"up\"\n",
    "            lead_y_change = -snake_speed\n",
    "            lead_x_change = 0\n",
    "        elif action == 3 and direction != \"up\":\n",
    "            direction = \"down\"\n",
    "            lead_y_change = snake_speed\n",
    "            lead_x_change = 0         \n",
    "\n",
    "        # Hits walls\n",
    "        if lead_x >= display_width - block_size or lead_x < 0 or lead_y >= display_height - block_size or lead_y < 0:\n",
    "            must_die = True\n",
    "            reward = death_reward\n",
    "#             gameOver = True\n",
    "\n",
    "        lead_x += lead_x_change\n",
    "        lead_y += lead_y_change\n",
    "\n",
    "        gameDisplay.fill(white)\n",
    "\n",
    "        \n",
    "        gameDisplay.blit(appleimg, (randAppleX, randAppleY))\n",
    "        \n",
    "        snakeHead = []\n",
    "        snakeHead.append(lead_x)\n",
    "        snakeHead.append(lead_y)\n",
    "        snakeList.append(snakeHead)\n",
    "\n",
    "        if len(snakeList) > snakeLength:\n",
    "            del snakeList[0]\n",
    "\n",
    "        # Hits itself\n",
    "        for eachSegment in snakeList[:-1]:\n",
    "            if eachSegment == snakeHead:\n",
    "#                 gameOver = True\n",
    "                must_die = True\n",
    "                reward = death_reward\n",
    "\n",
    "        snake(block_size, snakeList)\n",
    "\n",
    "        Score(snakeLength-1)\n",
    "        \n",
    "        pygame.display.update()\n",
    "\n",
    "        # Eats apple\n",
    "        if lead_x > randAppleX and lead_x < randAppleX + AppleThickness or lead_x + block_size > randAppleX and lead_x + block_size < randAppleX + AppleThickness:\n",
    "            if lead_y > randAppleY and lead_y < randAppleY + AppleThickness:\n",
    "                randAppleX, randAppleY = randAppleGen()\n",
    "                snakeLength += 1\n",
    "                reward = apple_reward\n",
    "            elif lead_y + block_size > randAppleY and lead_y + block_size < randAppleY + AppleThickness:\n",
    "                randAppleX, randAppleY = randAppleGen()\n",
    "                snakeLength += 1\n",
    "                reward = apple_reward\n",
    "\n",
    "        # Update next state\n",
    "        next_state[1] = screenshot()\n",
    "\n",
    "        # Add SARS tuple to experience_buffer\n",
    "        experience_buffer.append((np.asarray([state]), action, reward,\n",
    "                                  np.asarray([next_state]),\n",
    "                                  True if must_die else False))\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Change current state\n",
    "        state = list(next_state)\n",
    "\n",
    "        # Poll the DQAgent to get the next action\n",
    "        action = DQA.get_action(np.asarray([state]), testing=False)\n",
    "\n",
    "        # Stopping condition\n",
    "        if must_die or episode_length > 5 * MAX_EPISODE_LENGTH_FACTOR: # need to change 5 to some generic number\n",
    "            die(DQA)\n",
    "            \n",
    "#         clock.tick(fps)  #setting the fps\n",
    "\n",
    "    pygame.quit()\n",
    "    quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sahil\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (2, 2), strides=(2, 2), padding=\"valid\", data_format=\"channels_first\")`\n",
      "  del sys.path[0]\n",
      "C:\\Users\\sahil\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (2, 2), strides=(2, 2), padding=\"valid\", data_format=\"channels_first\")`\n",
      "C:\\Users\\sahil\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), strides=(2, 2), padding=\"valid\", data_format=\"channels_first\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding episode to experiences - Score: 2; Episode length: 501\n",
      "Got 0 samples of 1024\n",
      "Adding episode to experiences - Score: 1; Episode length: 184\n",
      "Got 0 samples of 1024\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-b4369a0cdc29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Run the game\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# game_intro()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mgameLoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-40db5c0c4f6e>\u001b[0m in \u001b[0;36mgameLoop\u001b[1;34m()\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[1;31m# Update next state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m         \u001b[0mnext_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscreenshot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[1;31m# Add SARS tuple to experience_buffer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-30e9f95be6ae>\u001b[0m in \u001b[0;36mscreenshot\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0mgameDisplay\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgameDisplay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'RGB'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Take screenshot\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrombytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'RGB'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdisplay_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisplay_height\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'L'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Convert to greyscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSCREENSHOT_DIMS\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Resize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mfrombytes\u001b[1;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[0;32m   2365\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2367\u001b[1;33m     \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2368\u001b[0m     \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrombytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2369\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mnew\u001b[1;34m(mode, size, color)\u001b[0m\n\u001b[0;32m   2329\u001b[0m         \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageColor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcolor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2331\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run the game    \n",
    "# game_intro()\n",
    "gameLoop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
